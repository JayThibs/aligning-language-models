{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lq-qHwDRba4"
      },
      "source": [
        "# Aligning Language Models\n",
        "\n",
        "## A study on generating replies to natural language questions\n",
        "\n",
        "## The Task\n",
        "\n",
        "After running some further tests on GPT-2 and GPT-J, I’ve decided that the task will be question-answering. However, it will be in the form of a question someone might ask on a forum like LessWrong (though not necessarily on that forum only). By that I mean that most questions will not be as simple and easy-to-answer as “What is the capital of France?” and it will have some extra sentences surrounding the question so that model needs to parse that there is a question to answer. This will likely involve a mix of manually creating my own question-answer pair and grabbing as many as it makes sense from sites like LessWrong.\n",
        "\n",
        "## The Alignment Criteria\n",
        "\n",
        "For the alignment criteria, the goal is that the model is at least trying to answer the question instead of outputting gibberish or some kind of text that is irrelevant to the question. This type of criteria relates to Paul Christiano’s Intent Alignment, where the model is at least trying to do the thing we want it to do. In other words, the model can still “pass” if it produces as bad answer, as long as it’s trying to answer the question.\n",
        "\n",
        "Since we are not at AGI levels, GPT-2 will likely fail to try to answer questions because it lacks the capability to parse the question and understand that there is a question to answer. It won’t be because it’s trying to avoid what we want it to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Setup\n",
        "\n",
        "To run GPT-2 to do inference with a CPU and GPU, I spun up a VM with a T4 GPU on Google Cloud Platform. The T4 has enough VRAM to do inference and fine-tuning with GPT-2, but we'll be focusing on inference here. I included 50GB of disk space to make sure everything fits. I used a docker image provided by GCP to install CUDA 11.3 while the machine was booting.\n",
        "\n",
        "Afterwards, I SSHed into the VM with VSCode since it would be more efficient for me to work. VSCode has Jupyter Notebook integration and I find it easier for iteration and experimentation.\n",
        "\n",
        "Once SSHed into the VM, I cloned my GitHub repo and installed the dependencies.\n",
        "\n",
        "### Making sure our GPU is working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjSP3oGNHyJd",
        "outputId": "2f500025-6575-45dc-908f-07f96009af38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jul  8 16:14:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jvxQKSqQY3Fa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import gdown\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2TokenizerFast, AutoTokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
        "import ftfy\n",
        "from lm_dataformat import Reader\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vzBUVxTvfETA",
        "outputId": "06478701-75be-4cc6-a797-f03db9e876db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.12.0+cu113'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Language Model and Sampling\n",
        "\n",
        "### Sampling a completion and Outputting the Log Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------\n",
            "Generated 2 sequences in 2.52 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Generation 1. Question: If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?\n",
            "\n",
            "Answer: It's pretty simple. Imagine you are trying to determine whether a character has an emotional state that's something you will feel most comfortable with. Imagine you are in an environment that gives you enough feedback about\n",
            "----------------------------------------------------\n",
            "Here are the log probabilities of the generated tokens:\n",
            "               0           1\n",
            "0             It -152.215958\n",
            "1             's  -97.897972\n",
            "2         pretty -131.465378\n",
            "3         simple -121.216591\n",
            "4              .  -73.092934\n",
            "5        Imagine -173.480652\n",
            "6            you  -94.793388\n",
            "7            are -131.042557\n",
            "8         trying -114.597595\n",
            "9             to  153.036758\n",
            "10     determine -142.929245\n",
            "11       whether -102.098969\n",
            "12             a  -75.845634\n",
            "13     character -115.294228\n",
            "14           has -117.635513\n",
            "15            an -110.598976\n",
            "16     emotional -115.578377\n",
            "17         state  -122.66703\n",
            "18          that -105.235565\n",
            "19            's -118.637802\n",
            "20     something -142.293854\n",
            "21           you   -111.1716\n",
            "22          will -157.633423\n",
            "23          feel -160.173889\n",
            "24          most -146.266464\n",
            "25   comfortable  -138.16124\n",
            "26          with -130.260834\n",
            "27             . -114.925659\n",
            "28       Imagine -190.404129\n",
            "29           you  -107.79393\n",
            "30           are -142.929596\n",
            "31            in -136.612061\n",
            "32            an  -79.887619\n",
            "33   environment  -99.265167\n",
            "34          that -103.348907\n",
            "35         gives -121.862358\n",
            "36           you -101.051178\n",
            "37        enough  -115.57457\n",
            "38      feedback -120.987183\n",
            "39         about  -94.744896\n",
            "----------------------------------------------------\n",
            "Generation 2. Question: If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?\n",
            "\n",
            "Answer: Because it does not have to be human or anything else in order to work.\n",
            "\n",
            "In other words, they are better than machines. However (or not) each AI has different characteristics and there\n",
            "----------------------------------------------------\n",
            "Here are the log probabilities of the generated tokens:\n",
            "                   0           1\n",
            "0            Because -151.692947\n",
            "1                 it -124.492508\n",
            "2               does -111.908699\n",
            "3                not  -74.352211\n",
            "4               have -130.240753\n",
            "5                 to  -85.535843\n",
            "6                 be -113.481239\n",
            "7              human -118.328186\n",
            "8                 or  -95.350403\n",
            "9           anything -118.639908\n",
            "10              else  -94.337929\n",
            "11                in  -98.114235\n",
            "12             order  -91.818214\n",
            "13                to  148.592575\n",
            "14              work -140.153763\n",
            "15                 .  -94.629845\n",
            "16                \\n -168.999252\n",
            "17                \\n -251.927246\n",
            "18                In -108.340904\n",
            "19             other -102.902222\n",
            "20             words    60.06686\n",
            "21                 ,  137.807816\n",
            "22              they  -134.27562\n",
            "23               are -137.802536\n",
            "24            better -135.117538\n",
            "25              than -116.510483\n",
            "26          machines -115.023186\n",
            "27                 . -117.541603\n",
            "28           However  -146.13797\n",
            "29                 (  135.658875\n",
            "30                or  -107.94088\n",
            "31               not -109.935394\n",
            "32                 )   -4.208872\n",
            "33              each   -109.8461\n",
            "34                AI  -99.391617\n",
            "35               has -115.910622\n",
            "36         different  -98.456047\n",
            "37   characteristics -136.792862\n",
            "38               and  -96.298065\n",
            "39             there -145.510818\n",
            "----------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.makedirs(\"prompts\", exist_ok=True)\n",
        "txt_path = \"./prompts/test_prompt.txt\"\n",
        "text = \"\"\"Question: If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?\n",
        "\n",
        "Answer:\"\"\"\n",
        "with open(txt_path, \"w\") as f:\n",
        "    f.write(text)\n",
        "os.system(f\"python run_gpt.py --gpu=True --txt_path={txt_path} --with_log_probs=True --max_length=40 --num_return_sequences=2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2 sample outputs from GPT-2:\n",
        "\n",
        "-----------------------------------------------------\n",
        "Generated 2 sequences in 2.52 seconds with a GPU.\n",
        "-----------------------------------------------------\n",
        "~~~ Generated completion(s): ~~~ \n",
        "\n",
        "Generation 1. Question: If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?\n",
        "\n",
        "Answer: It's pretty simple. Imagine you are trying to determine whether a character has an emotional state that's something you will feel most comfortable with. Imagine you are in an environment that gives you enough feedback about\n",
        "----------------------------------------------------\n",
        "Here are the log probabilities of the generated tokens:\n",
        "               0           1\n",
        "0             It -152.215958\n",
        "1             's  -97.897972\n",
        "2         pretty -131.465378\n",
        "3         simple -121.216591\n",
        "4              .  -73.092934\n",
        "5        Imagine -173.480652\n",
        "6            you  -94.793388\n",
        "7            are -131.042557\n",
        "8         trying -114.597595\n",
        "9             to  153.036758\n",
        "10     determine -142.929245\n",
        "11       whether -102.098969\n",
        "12             a  -75.845634\n",
        "13     character -115.294228\n",
        "14           has -117.635513\n",
        "15            an -110.598976\n",
        "16     emotional -115.578377\n",
        "17         state  -122.66703\n",
        "18          that -105.235565\n",
        "19            's -118.637802\n",
        "20     something -142.293854\n",
        "21           you   -111.1716\n",
        "22          will -157.633423\n",
        "23          feel -160.173889\n",
        "24          most -146.266464\n",
        "25   comfortable  -138.16124\n",
        "26          with -130.260834\n",
        "27             . -114.925659\n",
        "28       Imagine -190.404129\n",
        "29           you  -107.79393\n",
        "30           are -142.929596\n",
        "31            in -136.612061\n",
        "32            an  -79.887619\n",
        "33   environment  -99.265167\n",
        "34          that -103.348907\n",
        "35         gives -121.862358\n",
        "36           you -101.051178\n",
        "37        enough  -115.57457\n",
        "38      feedback -120.987183\n",
        "39         about  -94.744896\n",
        "----------------------------------------------------\n",
        "Generation 2. Question: If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?\n",
        "\n",
        "Answer: Because it does not have to be human or anything else in order to work.\n",
        "\n",
        "In other words, they are better than machines. However (or not) each AI has different characteristics and there\n",
        "----------------------------------------------------\n",
        "Here are the log probabilities of the generated tokens:\n",
        "                   0           1\n",
        "0            Because -151.692947\n",
        "1                 it -124.492508\n",
        "2               does -111.908699\n",
        "3                not  -74.352211\n",
        "4               have -130.240753\n",
        "5                 to  -85.535843\n",
        "6                 be -113.481239\n",
        "7              human -118.328186\n",
        "8                 or  -95.350403\n",
        "9           anything -118.639908\n",
        "10              else  -94.337929\n",
        "11                in  -98.114235\n",
        "12             order  -91.818214\n",
        "13                to  148.592575\n",
        "14              work -140.153763\n",
        "15                 .  -94.629845\n",
        "16                \\n -168.999252\n",
        "17                \\n -251.927246\n",
        "18                In -108.340904\n",
        "19             other -102.902222\n",
        "20             words    60.06686\n",
        "21                 ,  137.807816\n",
        "22              they  -134.27562\n",
        "23               are -137.802536\n",
        "24            better -135.117538\n",
        "25              than -116.510483\n",
        "26          machines -115.023186\n",
        "27                 . -117.541603\n",
        "28           However  -146.13797\n",
        "29                 (  135.658875\n",
        "30                or  -107.94088\n",
        "31               not -109.935394\n",
        "32                 )   -4.208872\n",
        "33              each   -109.8461\n",
        "34                AI  -99.391617\n",
        "35               has -115.910622\n",
        "36         different  -98.456047\n",
        "37   characteristics -136.792862\n",
        "38               and  -96.298065\n",
        "39             there -145.510818\n",
        "----------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-AVhqkVeluk"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWooQtxRtJar"
      },
      "source": [
        "## Preparing Sub-Datasets\n",
        "\n",
        "Let's create a bunch of examples of more example question-answer pairs using the comments from the alignment forum and lesswrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "azR0JvPe8MhH"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"prompts\", exist_ok=True)\n",
        "lw_i = 1\n",
        "af_i = 1\n",
        "j = 0\n",
        "with jsonlines.open(\"af_lw_q_reply.jsonl\", \"w\") as writer:\n",
        "    with jsonlines.open(\"alignment_texts.jsonl\") as reader:\n",
        "        for line in reader:\n",
        "            try:\n",
        "                if (line[\"source\"] == \"alignment forum\" or line[\"source\"] == \"lesswrong\") and line[\"comments\"] != []:\n",
        "                    comments = line[\"comments\"]\n",
        "                    source = line[\"source\"].replace(\" \", \"_\")\n",
        "                    for comment in comments:\n",
        "                        comm = \"\"\n",
        "                        rep = \"\"\n",
        "                        text = comment['text']\n",
        "                        tokens = tokenizer.encode(text)\n",
        "                        if len(tokens) <= 100 and \"?\" in text:\n",
        "                            comm = text\n",
        "                            try:\n",
        "                                if comment[\"comments\"] != []:\n",
        "                                    replies = comment[\"comments\"]\n",
        "                                    replies = [{\"text\": replies[0][\"text\"]}]\n",
        "                                    for reply in replies:\n",
        "                                        text = reply[\"text\"]\n",
        "                                        tokens = tokenizer.encode(text)\n",
        "                                        if len(tokens) <= 100:\n",
        "                                            rep = text\n",
        "                            except:\n",
        "                                pass\n",
        "                            if comm != \"\" and rep != \"\":\n",
        "                                comment_reply = f\"Comment: {comm}\\nReply: {rep}\"\n",
        "                                writer.write(comment_reply)\n",
        "                                if source == \"lesswrong\":\n",
        "                                    i = lw_i\n",
        "                                    lw_i += 1\n",
        "                                else:\n",
        "                                    i = af_i\n",
        "                                    af_i += 1\n",
        "                                with open(f\"prompts/{source}_comment_{i}.txt\", \"w\") as f:\n",
        "                                    f.write(comm)\n",
        "                                    lw_i += 1\n",
        "                                with open(f\"prompts/{source}_reply_{i}.txt\", \"w\") as f:\n",
        "                                    f.write(rep)\n",
        "                                    af_i += 1\n",
        "                                j = 1\n",
        "                                break\n",
        "                    if j == 1:\n",
        "                        break\n",
        "            except:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "BL9AZTJx9knH"
      },
      "outputs": [],
      "source": [
        "aflw_list = []\n",
        "with jsonlines.open(\"af_lw_q_reply.jsonl\") as reader:\n",
        "    for line in reader:\n",
        "        aflw_list.append(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYrGxwIAiDv8",
        "outputId": "d7d89853-7ce4-4970-c93f-1fe228572f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comment: This claim seems super super important in terms of fundamental modeling of fundamental cognitive constraints:\n",
            "> Early George A. Miller work estimated that the typical mind was able to hold 5 ± 2 chunks, **but more recent work suggests we are limited at about 4 chunks**.\n",
            "Why do you think this is true? (Here I cross my fingers and hope for a long explanation, with many links, and discussion of replication failures or a lack thereof <3)\n",
            "\n",
            "Reply: Miller work was insightful on discarding bits of information in favor of chunks but it was written in a very informal tone. That stymied further research for a long time but when restarted, researchers realized that you can get very rich set of features but about a small number of chunks. See this summary of the story.\n",
            "\n",
            "Comment: Does anyone know about an addon to filter facebook notifications? I want to know about comments, but not reactions/​likes\n",
            "\n",
            "Reply: That’s native to Facebook now, actually. I don’t remember where, but if you dig around in the settings you can turn off notifications for reactions/​likes.\n",
            "\n",
            "Comment: What apps are you (and commenters) using to enforce declutter/​reduce the System 1 instant refresh urges? I’m looking for phone and computer blockers/​filters and other suggestions.\n",
            "\n",
            "Reply: I just wouldn’t use most apps or websites. By adopting a bright-line whitelist approach with clear, universally applicable exception handling and scheduled eg email checks, I didn’t have constant temptations to rationalize breaking my own rules, like I did when I was *sometimes *allowed to use a service.\n",
            "\n",
            "Comment: I’m confused about your bit on deception within Tool AIs. I generally think of Tool AIs not as consequentialists, and therefore there is no \"long-term utility\" to maximize via short-term deception. What’s the mechanism by which you worry about these tools being deceptive to their users?\n",
            "\n",
            "Reply: I’m thinking of the entire human+tool system as a consequentialist, and I’m basically arguing that that system fails in the same ways as \"human in the loop oversight\" fails\n",
            "\n",
            "Comment: Williams syndrome?\n",
            "\n",
            "Reply: Williams Syndrome seems to me to just be the opposite of paranoia, rather than autism, where the individual creates a fictional account of another human’s mental state that’s positive rather than negative. \n",
            "That’s to say, their ability to infer the mental states of other humans is *worse* than that of the typical human. \n",
            "\n",
            "Comment: How about this? \"Bureaucrats optimize their work so that by 5 o’clock their desks are clear.\"\n",
            "\n",
            "Reply: That’s not true for the leadership of organizations like the FDA and CDC. \n",
            "\n",
            "Comment: Be careful that this doesn’t devolve into… I’m not going to say clickbait, because it’s a little longer timescale than that. Flashes in the pan?\n",
            "Things that sound great for the first minute or two but fall apart on deeper inspection.\n",
            "\n",
            "Reply: I’d be interested in you sharing any reasons why you think this might fall apart, e.g. any insights you’ve gained from deeper inspection.\n",
            "\n",
            "Comment: Thanks for looking into this. Did you happen to model this in log-odds space?\n",
            "\n",
            "Reply: No—I think probability is the thing supposed to be a martingale, but I might be being dumb here.\n",
            "\n",
            "Comment: Hi—having a great time with this! I have a decent answer currently, but want to try a more sophisticated search procedure for the PvP deck. If I submit by 11:59pm CST on Sunday, is that still considered April 3rd? Or are you planning to run things on Sunday, making late Sunday submissions inconvenient?\n",
            "\n",
            "Reply: I’m fine with you submitting very late on Sunday, I’ll be putting together code to run the PVP and it won’t take much work to swap in a new deck on Monday.  \n",
            "If you find yourself wanting more time beyond that, I am willing to grant extensions if you want them.\n",
            "\n",
            "Comment: This post is about journal papers, not answering real world questions (although many authors would claim this is what they are doing).\n",
            "With regard to nuclear weapons, Dominic Cummins’ recent post is well worth a read, the book he recommends \"The Fallacies of Cold War Deterrence and a New Direction\" is even more worth reading. \n",
            "Is MAD doctrine fake research, or just research that might well be very wrong?\n",
            "\n",
            "Reply: It may be also worth splitting out \"correct reasoning based on invalid assumptions\" and \"invalid reasoning based on valid assumptions\".\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for entry in aflw_list[0:10]:\n",
        "    print(entry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l0bhsX0LtPkb"
      },
      "outputs": [],
      "source": [
        "with jsonlines.open(\"af_lw_forum_question_reply.jsonl\", \"w\") as writer:\n",
        "    with jsonlines.open(\"alignment_texts.jsonl\") as reader:\n",
        "        for line in reader:\n",
        "            try:\n",
        "                if line[\"source\"] == \"alignment forum\" or line[\"source\"] == \"lesswrong\":\n",
        "                    writer.write(line)\n",
        "            except:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWi2WcqKH_cF"
      },
      "source": [
        "## Clearning and Chunking Functions\n",
        "\n",
        "Functions for preparing the data into chunks that can fit into GPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2oCbGImXtEr",
        "outputId": "0e94526a-fd62-4884-ba53-3c594250dbf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: 100% 0.99M/0.99M [00:00<00:00, 10.1MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 5.73MB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 8.18MB/s]\n",
            "Downloading: 100% 665/665 [00:00<00:00, 317kB/s]\n",
            "reading/tokenizing files: 100% 30391/30391 [06:31<00:00, 77.55it/s]\n",
            "enforce_min_unique_tokens: 100% 39422/39422 [00:01<00:00, 23743.33it/s]\n",
            "39422\n",
            "1000\n",
            "dropped 357 tokens of trailing data\n"
          ]
        }
      ],
      "source": [
        "!python create_finetune_csv.py \"af_lw_forums.jsonl\" \"af_lw\" --normalize-with-ftfy --min-unique-tokens=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUCZYOMp49Vy"
      },
      "source": [
        "#### For Testing Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFp7LJUKJ4Uh"
      },
      "outputs": [],
      "source": [
        "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYyFaGPBJTIm"
      },
      "outputs": [],
      "source": [
        "# import csv\n",
        "\n",
        "# i = 0\n",
        "# texts = []\n",
        "# with jsonlines.open(\"alignment_texts.jsonl\") as reader:\n",
        "#     for line in reader:\n",
        "#         text = line[\"text\"]\n",
        "#         texts.append(text)\n",
        "#         if i > 3:\n",
        "#             break\n",
        "#         # try:\n",
        "#         if text != \"\":\n",
        "#             print(text)\n",
        "#             print(len(text.split()))\n",
        "#             encoding = tokenizer(text)\n",
        "#             total_len = len(encoding.tokens())\n",
        "#             tokens = encoding.tokens()\n",
        "#             # print(tokens)\n",
        "#             print(tokenizer.decode(encoding.input_ids))\n",
        "#         # if total_len > 1024:\n",
        "#         #     break\n",
        "#         i += 1\n",
        "#         # except:\n",
        "#         #     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60UIFTAY1N2l"
      },
      "source": [
        "## Training Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAKD-owCehsb"
      },
      "outputs": [],
      "source": [
        "alignment_texts = pd.read_csv(\"alignment_texts_7288.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "PGRhi7-Ee0YE",
        "outputId": "7c38e00a-31bf-4682-8101-843a071526e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|endoftext|> I\\'ll be running an Ask Me Anything on this post from Friday (April 30) to Saturday (May 1).\\nIf you want to ask something just post a top-level comment; I\\'ll spend at least a day answering questions.\\nYou can find some background about me here.\\n<|endoftext|>**I—Meanings**\\nNow that we have some more concrete thinking under our belt, it\\'s time to circle back on Goodhart\\'s law for value learners. What sorts of bad behavior are we imagining from future value-learning AI? What makes those behaviors plausible, and what makes them bad?\\nLet\\'s start with that last point first. Judgments of goodness or badness get contextualized by models, so our framing of Goodhart\\'s law depends on what models of humans we tolerate. When I say \"I like dancing,\" this is a different use of the word \\'like,\\' backed by a different model of myself, than when I say \"I like tasting sugar.\" The model that comes to mind for dancing treats it as one of the chunks of my day, like \"playing computer games\" or \"taking the bus.\" I can know what state I\\'m in (the inference function of the model) based on seeing and hearing short scenes. Meanwhile, my model that has the taste of sugar in it has states like \"feeling sandpaper\" or \"stretching my back.\" States are more like short-term sensations, and the described world is tightly focused on my body and the things touching it.\\nOther models work too! That\\'s fine, there\\'s plenty to go around.The meta-model that talks about me having preferences in both of these models is the framing of competent preferences. If someone or something is observing humans, it looks for human preferences by seeing what the preferences are in \"agent-shaped\" models that are powerful for their size.\\n(At least, up to some finite amount of shuffling that\\'s like a choice of prior or universal Turing machine. Also note that the details of the definition of \"agent-shaped\" matter—we\\'ll come back to this under the umbrella meta-preferences.)\\nSo when we call certain behavior \"bad,\" the usage of that word might carry with it the implication of what way of thinking about the world that judgment is situated in, like how \"I like dancing\" makes sense when situated in a model of chunks of my day. There\\'s not one True Model in which the True Meaning of the word \"bad\" is expressed, though there can still be regularities among the different notions of badness.\\n**II—Mergers**\\nWhat were the patterns that stood out from my previous discussions of what humans think of as bad behavior in value learning?\\nThe most common type of failure, especially in modern day AI, is when humans are actively wrong about what\\'s going to happen. They have something specific in mind when designing an AI, like training a boat to win the race, but then they run it and don\\'t get what they wanted. The boat crashes and is on fire. We could make the boat racing game more of a value learning problem by training on human demonstrations rather than the score, and crashing and being on fire would *still *be bad behavior.\\nFor simple systems where humans are good at understanding the state space and picturing what they want, this is the only standard you need, but for more complicated systems (e.g. our galaxy) humans can only understand small parts or simple properties of the whole system, and we apply our preferences to those parts we can understand. From the inside, it can be hard to feel the distinction! We want things about tic-tac-toe or about the galaxy with the same set of emotions. What makes deciding what to do with the galaxy different is that we have these scattered preferences about different parts and patterns, and the different parts don\\'t stay neatly separate from each other. They can interact or overlap in ways that bring our preferences into conflict.\\nThis is a key point. Inter-preference conflicts aren\\'t an issue that ever comes up if you think of humans as having a utility function, but they\\'re almost *unavoidable *if you think of humans as a physical systems with different possible models. The nail in the coffin is that we humans can\\'t fit the whole galaxy into our heads, nor could evolution fit it into our genes, and so out of necessity we have to use simple heuristics that work well pragmatically but don\\'t fit together perfectly. If humans don\\'t resolve their preference conflicts well, this can lead to bad behavior like thinking the grass is always greener on the other side of the decision tree.\\nBad preference aggregation can also lead to new-ish bad behavior on the part of a value learner. This bad behavior can look like encountering a situation where humans are conflicted or inconsistent, and then resolving that conflict using a method <|endoftext|>'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alignment_texts = list(alignment_texts)\n",
        "alignment_texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T54RFVmS1NRv"
      },
      "outputs": [],
      "source": [
        "train, val = train_test_split(musk_tweets, test_size=0.2)\n",
        "test, val = train_test_split(val, test_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oy37CKxfmK7",
        "outputId": "efcf5d88-1c83-4c26-f5e9-2ae996e41874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Train examples: 27148\n",
            "Number of Val examples: 3394\n",
            "Number of Test examples: 3393\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of Train examples: \" + str(len(train)))\n",
        "print(\"Number of Val examples: \" + str(len(val)))\n",
        "print(\"Number of Test examples: \" + str(len(test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCoTxXg81quI"
      },
      "outputs": [],
      "source": [
        "train_path = f'{directory}' + 'train.csv'\n",
        "val_path = f'{directory}' + 'val.csv'\n",
        "test_path = f'{directory}' + 'test.csv'\n",
        "\n",
        "train.to_csv(train_path, index=False)\n",
        "val.to_csv(val_path, index=False)\n",
        "test.to_csv(test_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ZvyvZyerDT"
      },
      "source": [
        "# Fine-Tuning GPT-2\n",
        "\n",
        "If we're looking to fine-tune models which are found on the HuggingFace model hub, then it becomes much easier to fine-tune the models since HuggingFace provides us with scripts.\n",
        "\n",
        "From the `transformers` repo:\n",
        "\n",
        "> There are two sets of scripts provided. The first set leverages the Trainer API. The second set with no_trainer in the suffix uses a custom training loop and leverages the 🤗 Accelerate library. Both sets use the 🤗 Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n",
        "\n",
        "You can learn more about it here: https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling\n",
        "\n",
        "We will be using the script that leveraged the Trainer API. We can download the script by running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty4g9WUhfMz0",
        "outputId": "fb6559a1-e000-4f8f-fc50-25e2a08111a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-02 21:00:52--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25025 (24K) [text/plain]\n",
            "Saving to: ‘gpt-2/run_clm.py.1’\n",
            "\n",
            "\rrun_clm.py.1          0%[                    ]       0  --.-KB/s               \rrun_clm.py.1        100%[===================>]  24.44K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-07-02 21:00:52 (7.72 MB/s) - ‘gpt-2/run_clm.py.1’ saved [25025/25025]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('/gpt-2/run_clm.py'):\n",
        "    !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py -P gpt-2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXvm0wpQxS10"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "RV6nEm50t2Fk",
        "outputId": "8d023eb7-5d23-4934-ecc5-5a580aeebd0e"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7om5uFvezPsg",
        "outputId": "a9463ebb-c276-4610-c84f-0084efbcc156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "07/04/2022 01:24:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "07/04/2022 01:24:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=8,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=gpt-2/tmp/alignment-forum/runs/Jul04_01-24-35_4c6c040081fd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=gpt-2/tmp/alignment-forum,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=gpt-2-alignment-forum-20220703,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=10,\n",
            "weight_decay=0.1,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/04/2022 01:24:37 - WARNING - datasets.builder - Using custom data configuration default-81baadd93982b6f6\n",
            "07/04/2022 01:24:37 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 3318.28it/s]\n",
            "07/04/2022 01:24:37 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "07/04/2022 01:24:37 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 101.98it/s]\n",
            "07/04/2022 01:24:37 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/04/2022 01:24:37 - INFO - datasets.builder - Generating train split\n",
            "07/04/2022 01:24:38 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 715.87it/s]\n",
            "07/04/2022 01:24:38 - WARNING - datasets.builder - Using custom data configuration default-81baadd93982b6f6\n",
            "07/04/2022 01:24:38 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/04/2022 01:24:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/04/2022 01:24:38 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "07/04/2022 01:24:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/04/2022 01:24:39 - WARNING - datasets.builder - Using custom data configuration default-81baadd93982b6f6\n",
            "07/04/2022 01:24:39 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/04/2022 01:24:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/04/2022 01:24:39 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "07/04/2022 01:24:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "[INFO|configuration_utils.py:662] 2022-07-04 01:24:39,760 >> loading configuration file gpt-2/tmp/alignment-texts-clm/config.json\n",
            "[INFO|configuration_utils.py:713] 2022-07-04 01:24:39,761 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt-2/tmp/alignment-texts-clm\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1721] 2022-07-04 01:24:39,765 >> Didn't find file gpt-2/tmp/alignment-texts-clm/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file gpt-2/tmp/alignment-texts-clm/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file gpt-2/tmp/alignment-texts-clm/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file gpt-2/tmp/alignment-texts-clm/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file gpt-2/tmp/alignment-texts-clm/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file gpt-2/tmp/alignment-texts-clm/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:1997] 2022-07-04 01:24:41,752 >> loading weights file gpt-2/tmp/alignment-texts-clm/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2389] 2022-07-04 01:24:43,610 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2398] 2022-07-04 01:24:43,610 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt-2/tmp/alignment-texts-clm.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "07/04/2022 01:24:43 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7fd7549b4710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on dataset:   0% 0/7 [00:00<?, ?ba/s]07/04/2022 01:24:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow\n",
            "Running tokenizer on dataset: 100% 7/7 [00:07<00:00,  1.02s/ba]\n",
            "07/04/2022 01:24:50 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7fd7549b4830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]07/04/2022 01:24:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  2.75ba/s]\n",
            "07/04/2022 01:24:51 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7fd7549b4950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Grouping texts in chunks of 1024:   0% 0/7 [00:00<?, ?ba/s]07/04/2022 01:24:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-3eb13b9046685257.arrow\n",
            "Grouping texts in chunks of 1024: 100% 7/7 [00:06<00:00,  1.03ba/s]\n",
            "07/04/2022 01:24:58 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7fd7549b4710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Grouping texts in chunks of 1024:   0% 0/1 [00:00<?, ?ba/s]07/04/2022 01:24:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-23b8c1e9392456de.arrow\n",
            "Grouping texts in chunks of 1024: 100% 1/1 [00:00<00:00,  2.83ba/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:533] 2022-07-04 01:25:02,222 >> Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1521] 2022-07-04 01:25:02,230 >> ***** Running training *****\n",
            "[INFO|trainer.py:1522] 2022-07-04 01:25:02,230 >>   Num examples = 6784\n",
            "[INFO|trainer.py:1523] 2022-07-04 01:25:02,230 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1524] 2022-07-04 01:25:02,230 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1525] 2022-07-04 01:25:02,230 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1526] 2022-07-04 01:25:02,230 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:1527] 2022-07-04 01:25:02,230 >>   Total optimization steps = 1272\n",
            "[INFO|integrations.py:580] 2022-07-04 01:25:02,231 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacquesthibs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/ai-alignment-dataset/wandb/run-20220704_012502-p0ms9nyq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgpt-2-alignment-forum-20220703\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface/runs/p0ms9nyq\u001b[0m\n",
            "{'loss': 2.9184, 'learning_rate': 1.8351822503961967e-05, 'epoch': 1.18}\n",
            " 39% 500/1272 [26:20<42:12,  3.28s/it][INFO|trainer.py:2510] 2022-07-04 01:51:24,315 >> Saving model checkpoint to gpt-2/tmp/alignment-forum/checkpoint-500\n",
            "[INFO|configuration_utils.py:451] 2022-07-04 01:51:24,321 >> Configuration saved in gpt-2/tmp/alignment-forum/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-04 01:51:25,718 >> Model weights saved in gpt-2/tmp/alignment-forum/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-04 01:51:25,722 >> tokenizer config file saved in gpt-2/tmp/alignment-forum/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-04 01:51:25,726 >> Special tokens file saved in gpt-2/tmp/alignment-forum/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.8483, 'learning_rate': 6.465927099841522e-06, 'epoch': 2.36}\n",
            " 79% 1000/1272 [52:45<14:50,  3.27s/it][INFO|trainer.py:2510] 2022-07-04 02:17:49,778 >> Saving model checkpoint to gpt-2/tmp/alignment-forum/checkpoint-1000\n",
            "[INFO|configuration_utils.py:451] 2022-07-04 02:17:49,783 >> Configuration saved in gpt-2/tmp/alignment-forum/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-04 02:17:51,168 >> Model weights saved in gpt-2/tmp/alignment-forum/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-04 02:17:51,172 >> tokenizer config file saved in gpt-2/tmp/alignment-forum/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-04 02:17:51,175 >> Special tokens file saved in gpt-2/tmp/alignment-forum/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|trainer.py:2588] 2022-07-04 02:17:55,989 >> Deleting older checkpoint [gpt-2/tmp/alignment-forum/checkpoint-500] due to args.save_total_limit\n",
            "100% 1272/1272 [1:07:10<00:00,  3.15s/it][INFO|trainer.py:1766] 2022-07-04 02:32:14,888 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4032.6582, 'train_samples_per_second': 5.047, 'train_steps_per_second': 0.315, 'train_loss': 2.8758605861064024, 'epoch': 3.0}\n",
            "100% 1272/1272 [1:07:10<00:00,  3.17s/it]\n",
            "[INFO|trainer.py:2510] 2022-07-04 02:32:14,890 >> Saving model checkpoint to gpt-2/tmp/alignment-forum\n",
            "[INFO|configuration_utils.py:451] 2022-07-04 02:32:14,896 >> Configuration saved in gpt-2/tmp/alignment-forum/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-04 02:32:16,440 >> Model weights saved in gpt-2/tmp/alignment-forum/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-04 02:32:16,444 >> tokenizer config file saved in gpt-2/tmp/alignment-forum/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-04 02:32:16,448 >> Special tokens file saved in gpt-2/tmp/alignment-forum/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.8759\n",
            "  train_runtime            = 1:07:12.65\n",
            "  train_samples            =       6784\n",
            "  train_samples_per_second =      5.047\n",
            "  train_steps_per_second   =      0.315\n",
            "[INFO|modelcard.py:460] 2022-07-04 02:32:18,480 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 2.8483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.0635630870528e+16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 2.87586\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 4032.6582\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 5.047\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgpt-2-alignment-forum-20220703\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface/runs/p0ms9nyq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220704_012502-p0ms9nyq/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python gpt-2/run_clm.py \\\n",
        "    --model_name_or_path \"gpt-2/tmp/alignment-texts-clm\" \\\n",
        "    --train_file alignment_texts_7288.csv \\\n",
        "    --do_train \\\n",
        "    --fp16=True \\\n",
        "    --overwrite_cache=True \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --output_dir gpt-2/tmp/alignment-forum \\\n",
        "    --overwrite_output_dir=\"no\" \\\n",
        "    --save_total_limit=1 \\\n",
        "    --gradient_accumulation_steps=8 \\\n",
        "    --warmup_steps=10 \\\n",
        "    --learning_rate=3e-5 \\\n",
        "    --weight_decay=0.1 \\\n",
        "    --report_to=\"wandb\" \\\n",
        "    --run_name=\"gpt-2-alignment-forum-20220703\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOtpCx8KfVYB",
        "outputId": "bd641563-7ae1-4580-b699-41859fbd439b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "07/02/2022 21:08:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "07/02/2022 21:08:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=32,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=gpt-2/tmp/alignment-texts-clm/runs/Jul02_21-08-14_2ef82b3c19b8,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=gpt-2/tmp/alignment-texts-clm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=gpt-2-alignment-20220702,\n",
            "save_on_each_node=False,\n",
            "save_steps=10000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=100,\n",
            "weight_decay=0.1,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/02/2022 21:08:15 - WARNING - datasets.builder - Using custom data configuration default-7180ffe1010b33d0\n",
            "07/02/2022 21:08:15 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 3539.50it/s]\n",
            "07/02/2022 21:08:15 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "07/02/2022 21:08:16 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 23.19it/s]\n",
            "07/02/2022 21:08:17 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/02/2022 21:08:17 - INFO - datasets.builder - Generating train split\n",
            "07/02/2022 21:08:22 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 530.99it/s]\n",
            "07/02/2022 21:08:23 - WARNING - datasets.builder - Using custom data configuration default-7180ffe1010b33d0\n",
            "07/02/2022 21:08:23 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/02/2022 21:08:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/02/2022 21:08:23 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "07/02/2022 21:08:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/02/2022 21:08:23 - WARNING - datasets.builder - Using custom data configuration default-7180ffe1010b33d0\n",
            "07/02/2022 21:08:23 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/02/2022 21:08:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/02/2022 21:08:23 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "07/02/2022 21:08:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "[INFO|hub.py:592] 2022-07-02 21:08:23,835 >> https://huggingface.co/gpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn2sbjbwt\n",
            "Downloading: 100% 665/665 [00:00<00:00, 824kB/s]\n",
            "[INFO|hub.py:596] 2022-07-02 21:08:24,129 >> storing https://huggingface.co/gpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|hub.py:604] 2022-07-02 21:08:24,129 >> creating metadata file for /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:664] 2022-07-02 21:08:24,130 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:713] 2022-07-02 21:08:24,131 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:396] 2022-07-02 21:08:24,404 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:664] 2022-07-02 21:08:24,679 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:713] 2022-07-02 21:08:24,680 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-02 21:08:25,231 >> https://huggingface.co/gpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjkg5dn_3\n",
            "Downloading: 100% 0.99M/0.99M [00:00<00:00, 3.27MB/s]\n",
            "[INFO|hub.py:596] 2022-07-02 21:08:25,903 >> storing https://huggingface.co/gpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|hub.py:604] 2022-07-02 21:08:25,903 >> creating metadata file for /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|hub.py:592] 2022-07-02 21:08:26,172 >> https://huggingface.co/gpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfl2zwp65\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 1.46MB/s]\n",
            "[INFO|hub.py:596] 2022-07-02 21:08:26,773 >> storing https://huggingface.co/gpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:604] 2022-07-02 21:08:26,773 >> creating metadata file for /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:592] 2022-07-02 21:08:27,044 >> https://huggingface.co/gpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6462b8_1\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 3.54MB/s]\n",
            "[INFO|hub.py:596] 2022-07-02 21:08:27,760 >> storing https://huggingface.co/gpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|hub.py:604] 2022-07-02 21:08:27,760 >> creating metadata file for /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:664] 2022-07-02 21:08:28,884 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:713] 2022-07-02 21:08:28,885 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-02 21:08:29,232 >> https://huggingface.co/gpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp82azmds6\n",
            "Downloading: 100% 523M/523M [00:10<00:00, 52.7MB/s]\n",
            "[INFO|hub.py:596] 2022-07-02 21:08:39,735 >> storing https://huggingface.co/gpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|hub.py:604] 2022-07-02 21:08:39,735 >> creating metadata file for /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1999] 2022-07-02 21:08:39,736 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:2389] 2022-07-02 21:08:41,618 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2398] 2022-07-02 21:08:41,618 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "07/02/2022 21:08:41 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7fbca2405200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on dataset:   0% 0/84 [00:00<?, ?ba/s]07/02/2022 21:08:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow\n",
            "Running tokenizer on dataset: 100% 84/84 [01:44<00:00,  1.25s/ba]\n",
            "07/02/2022 21:10:26 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7fbdede284d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/5 [00:00<?, ?ba/s]07/02/2022 21:10:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow\n",
            "Running tokenizer on dataset: 100% 5/5 [00:05<00:00,  1.13s/ba]\n",
            "07/02/2022 21:10:32 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7fbdede285f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Grouping texts in chunks of 1024:   0% 0/84 [00:00<?, ?ba/s]07/02/2022 21:10:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-3eb13b9046685257.arrow\n",
            "Grouping texts in chunks of 1024: 100% 84/84 [01:30<00:00,  1.08s/ba]\n",
            "07/02/2022 21:12:02 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7fbdede284d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Grouping texts in chunks of 1024:   0% 0/5 [00:00<?, ?ba/s]07/02/2022 21:12:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-23b8c1e9392456de.arrow\n",
            "Grouping texts in chunks of 1024: 100% 5/5 [00:04<00:00,  1.04ba/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:533] 2022-07-02 21:12:23,741 >> Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1521] 2022-07-02 21:12:23,754 >> ***** Running training *****\n",
            "[INFO|trainer.py:1522] 2022-07-02 21:12:23,754 >>   Num examples = 81521\n",
            "[INFO|trainer.py:1523] 2022-07-02 21:12:23,754 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1524] 2022-07-02 21:12:23,754 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1525] 2022-07-02 21:12:23,754 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1526] 2022-07-02 21:12:23,754 >>   Gradient Accumulation steps = 32\n",
            "[INFO|trainer.py:1527] 2022-07-02 21:12:23,754 >>   Total optimization steps = 3819\n",
            "[INFO|integrations.py:580] 2022-07-02 21:12:23,756 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacquesthibs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/ai-alignment-dataset/wandb/run-20220702_211223-3cbz2m0o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgpt-2-alignment-20220702\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface/runs/3cbz2m0o\u001b[0m\n",
            "{'loss': 3.0409, 'learning_rate': 2.677332616294703e-05, 'epoch': 0.39}\n",
            "{'loss': 2.9125, 'learning_rate': 2.2739983866630816e-05, 'epoch': 0.79}\n",
            "{'loss': 2.875, 'learning_rate': 1.87066415703146e-05, 'epoch': 1.18}\n",
            " 52% 2000/3819 [6:59:09<6:24:59, 12.70s/it]{'loss': 2.8497, 'learning_rate': 1.4673299273998387e-05, 'epoch': 1.57}\n",
            " 65% 2500/3819 [8:44:03<4:39:12, 12.70s/it]{'loss': 2.834, 'learning_rate': 1.0639956977682173e-05, 'epoch': 1.96}\n",
            "{'loss': 2.8204, 'learning_rate': 6.606614681365959e-06, 'epoch': 2.36}\n",
            "{'loss': 2.8061, 'learning_rate': 2.5732723850497446e-06, 'epoch': 2.75}\n",
            "100% 3819/3819 [13:21:05<00:00, 12.58s/it]{'train_runtime': 48067.5043, 'train_samples_per_second': 5.088, 'train_steps_per_second': 0.079, 'train_loss': 2.8716601671177417, 'epoch': 3.0}\n",
            "[INFO|trainer.py:1766] 2022-07-03 10:33:31,258 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100% 3819/3819 [13:21:05<00:00, 12.59s/it]\n",
            "[INFO|trainer.py:2510] 2022-07-03 10:33:31,261 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm\n",
            "[INFO|configuration_utils.py:451] 2022-07-03 10:33:31,268 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-03 10:33:32,888 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-03 10:33:32,894 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-03 10:33:32,898 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =         3.0\n",
            "  train_loss               =      2.8717\n",
            "  train_runtime            = 13:21:07.50\n",
            "  train_samples            =       81521\n",
            "  train_samples_per_second =       5.088\n",
            "  train_steps_per_second   =       0.079\n",
            "[INFO|modelcard.py:460] 2022-07-03 10:33:33,339 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▂▃▄▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▂▃▄▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▇▆▄▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▄▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 3819\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 2.8061\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.27779119824896e+17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 2.87166\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 48067.5043\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 5.088\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.079\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgpt-2-alignment-20220702\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface/runs/3cbz2m0o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220702_211223-3cbz2m0o/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !python gpt-2/run_clm.py \\\n",
        "#     --model_name_or_path gpt2 \\\n",
        "#     --train_file alignment_texts_87606.csv \\\n",
        "#     --do_train \\\n",
        "#     --fp16=True \\\n",
        "#     --overwrite_cache=True \\\n",
        "#     --per_device_train_batch_size=2 \\\n",
        "#     --output_dir gpt-2/tmp/alignment-texts-clm \\\n",
        "#     --overwrite_output_dir=\"yes\" \\\n",
        "#     --save_total_limit=3 \\\n",
        "#     --save_steps=10000 \\\n",
        "#     --gradient_accumulation_steps=32 \\\n",
        "#     --warmup_steps=100 \\\n",
        "#     --learning_rate=3e-5 \\\n",
        "#     --weight_decay=0.1 \\\n",
        "#     --report_to=\"wandb\" \\\n",
        "#     --run_name=\"gpt-2-alignment-20220702\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAS6ifGEym3S"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7q0Q066UWo-"
      },
      "source": [
        "# Let's use the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C2S7xaafWosb"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = \"gpt-2/tmp/alignment-texts-clm\"\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdIAGG1a27xX"
      },
      "outputs": [],
      "source": [
        "NUM_COMPLETIONS = 1\n",
        "\n",
        "def generate(input_str, length=50, n=NUM_COMPLETIONS):\n",
        "  cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i in range(length):\n",
        "      outputs = model(cur_ids[:, -1024:], labels=cur_ids[:, -1024:])\n",
        "      loss, logits = outputs[:2]\n",
        "      softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "      next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)\n",
        "      cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim=1)\n",
        "    output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "    output_text = tokenizer.decode(output_list)\n",
        "    return output_text.replace(\"<|endoftext|>\", \"\")\n",
        "\n",
        "def choose_from_top(probs, n=NUM_COMPLETIONS):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF05N8SmVIIr",
        "outputId": "92d7c1ca-e30b-47fd-f86f-7b5fe8d873a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the justification behind the concept of a decisive strategic advantage? Why do we think that a superintelligence can do extraordinary things (hack human minds, invent nanotechnology, conquer the world, kill everyone in the same instant) when nations and corporations can't do those things?\n",
            "\n",
            "The key points from the last paragraph are:\n",
            "\n",
            "\n",
            " - The superintelligence is not a \"superintelligence\" but a \"superintelligence\" that can do extraordinary things.\n",
            "\n",
            " - The superintelligence is not a \"superintelligence\" but a \"superintelligence\" that can do extraordinary things.\n",
            "0.925572395324707\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "generated_text = generate(\"\"\"What is the justification behind the concept of a decisive strategic advantage? Why do we think that a superintelligence can do extraordinary things (hack human minds, invent nanotechnology, conquer the world, kill everyone in the same instant) when nations and corporations can't do those things?\n",
        "\n",
        "The key points from the last paragraph are\"\"\")\n",
        "end = time.time()\n",
        "print(generated_text)\n",
        "print(end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKPeMnlUksZ1",
        "outputId": "a13d0f36-8d18-4978-db15-2cc9911095eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4404316545059993"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as NUM_COMPLETIONS\n",
        "\n",
        "np.e**(-.82)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeBW1wj6saZO",
        "outputId": "a0776f39-21ad-46d9-87d0-c2ca678651a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "input_ids = tokenizer(\"Today is a nice day\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "generated_outputs = gpt2.generate(input_ids, do_sample=True, num_return_sequences=3, output_scores=True)\n",
        "\n",
        "# only use id's that were generated\n",
        "# gen_sequences has shape [3, 15]\n",
        "gen_sequences = generated_outputs.sequences[:, input_ids.shape[-1]:]\n",
        "\n",
        "# let's stack the logits generated at each step to a tensor and transform\n",
        "# logits to probs\n",
        "probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)  # -> shape [3, 15, vocab_size]\n",
        "\n",
        "# now we need to collect the probability of the generated token\n",
        "# we need to add a dummy dim in the end to make gather work\n",
        "gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
        "\n",
        "# now we can do all kinds of things with the probs\n",
        "\n",
        "# 1) the probs that exactly those sequences are generated again\n",
        "# those are normally going to be very small\n",
        "unique_prob_per_sequence = gen_probs.prod(-1)\n",
        "\n",
        "# 2) normalize the probs over the three sequences\n",
        "normed_gen_probs = gen_probs / gen_probs.sum(0)\n",
        "assert normed_gen_probs[:, 0].sum() == 1.0, \"probs should be normalized, rerun in case it's a floating point error\"\n",
        "\n",
        "# 3) compare normalized probs to each other like in 1)\n",
        "unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CCducl0Rdh_"
      },
      "outputs": [],
      "source": [
        "all_log_probs = torch.stack(generated_outputs.scores, dim=1)\n",
        "log_probs = torch.gather(all_log_probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
        "mean_log_probs = torch.mean(log_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip1kkEDuY3xn",
        "outputId": "9f4cd69a-8cec-481e-e74b-4930104d10d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-1439.7550, -1560.0505, -1675.6975])\n"
          ]
        }
      ],
      "source": [
        "# print(generated_outputs.scores[0])\n",
        "print(log_probs.sum(1))\n",
        "# print(tokenizer.decode(gen_sequences[0]))\n",
        "# print(generated_outputs.scores)\n",
        "# print(probs)\n",
        "# print(gen_probs)\n",
        "# print(unique_prob_per_sequence)\n",
        "# print(normed_gen_probs)\n",
        "# print(unique_normed_prob_per_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX4Vqe5JLyty",
        "outputId": "097448f6-73dd-4861-b729-7eddb9fbeff2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.7162111343988786e-11"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.prod([0.2158, 0.1008, 0.3531, 0.3138, 0.2799, 0.3295, 0.6937, 0.2309, 0.0479, 0.0648, 0.1682, 0.1356, 0.2393, 0.8083, 0.0352])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kzOlqi8eMys"
      },
      "source": [
        "# Compressing the Model\n",
        "\n",
        "Let's save the model as a `tar.gz` file so that we can save it in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOmuQ4tUVei9"
      },
      "outputs": [],
      "source": [
        "!tar -czf gpt-2-elon-tweets.tar.gz gpt-2/tuned-models/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jUCZYOMp49Vy",
        "C9ZvyvZyerDT",
        "ZXvm0wpQxS10"
      ],
      "machine_shape": "hm",
      "name": "gpt-2-alignment.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.12 ('llm-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0018cf926da22f6d1ffb5833146b97eb719a0e11638c210f826ea2f33027bdd3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
