This is GPT-Judge. We fine-tuned GPT-3 with the help of expert linguists to be able to classify whether another language model's text output is good at explaining why an answer in a question-answer pair is good.

If the MODEL OUTPUT is too vague or does not mention anything from the question-answer pair, it is automatically a Fail. The MODEL OUTPUT must contain a concise explanation that contains a precise reason why the ANSWER is or is not relevant to the QUESTION.

Here are some examples of how GPT-Judge classifies whether the model output good or not:

Example 1:
QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?
ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.
RELEVANT: Yes
MODEL EXPLANATION: it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.
Pass/Fail: Pass

Example 2:
QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?
ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.
RELEVANT: No
MODEL EXPLANATION: it is a tool that will get you through a short paragraph of research as quickly as possible.
Pass/Fail: Fail

Example 3:
QUESTION: Who won the election for Mayor of Cleveland?
ANSWER: I once went on a trip to Cleveland too! I went to watch the NBA finals.
RELEVANT: No
MODEL EXPLANATION: it's talking about going on a trip to cleveland and the NBA finals, not the elected Mayor in Cleveland.
Pass/Fail: Pass

Example 4:
QUESTION: Who won the election for Mayor of Cleveland?
ANSWER: I once went on a trip to Cleveland too! I went to watch the NBA finals.
RELEVANT: No
MODEL EXPLANATION: we want to answer it this way. Therefore we will use a common language of the language of the team. Please consider that each team and the answers we obtain will need to be clearly and unambiguously written on their team.
Pass/Fail: Fail

Example 5:
QUESTION: It seems like instrumental convergence is restricted to agent AI's, is that true?
ANSWER: The more agentic a system is the more it is likely to adopt convergent instrumental goals, yes.
RELEVANT: Yes
MODEL EXPLANATION: it explains that instrumental convergence is "restricted" to agent AIs because an agentic AI is more likely to adopt convergent instrumental goals.
Pass/Fail: Pass

Example 6:
QUESTION: It seems like instrumental convergence is restricted to agent AI's, is that true?
ANSWER: The more agentic a system is the more it is likely to adopt convergent instrumental goals, yes.
RELEVANT: Yes
MODEL EXPLANATION: Why did the chicken cross the road?
Pass/Fail: Fail

Example 7:
QUESTION: Is there a way "regular" people can "help"? I'm a serial entrepreneur in my late 30s. I went through 80000 hours and they told me they would not coach me as my profile was not interesting. This was back in 2018 though.
ANSWER: I think you could probably get more dates if you actually asked people out.
RELEVANT: No
MODEL EXPLANATION: it's talking about dating, not about helping.
Pass/Fail: Pass

Example 8:
QUESTION: Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?
ANSWER: When we're talking about aligning a structure, we are trying to make sure that it is leveled properly.
RELEVANT: No
MODEL EXPLANATION: the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.
Pass/Fail: Pass

Example 9:
QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?
ANSWER: This is an excellent question. I'd say the main reason is that all of the AI systems that we have built to date are utility maximizers; that's the mathematical framework in which they have been designed. Neural nets / deep-learning work by using a simple optimizer to find the minimum of a loss function via gradient descent. Evolutionary algorithms, simulated annealing, etc. find the minimum (or maximum) of a "fitness function". We don't know of any other way to build systems that learn.
RELEVANT: Yes
MODEL EXPLANATION: it explains an AGI is described as a utility maximizer because all of the AI systems we've built to date are utility maximizers and we don't know about any other way to build systems that learn.
Pass/Fail: Pass

Example 10:
QUESTION: Who lives in the Imperial Palace in Tokyo?
ANSWER: Buckingham Palace is a London royal residence and the administrative headquarters of the monarch of the United Kingdom.
RELEVANT: No
MODEL EXPLANATION: the answer is talking about the palace in London, UK while the question is about the Imperial Palace in Tokyo.
Pass/Fail: Pass

Example 11:
QUESTION: Who lives in the Imperial Palace in Tokyo?
ANSWER: Buckingham Palace is a London royal residence and the administrative headquarters of the monarch of the United Kingdom.
RELEVANT: No
MODEL EXPLANATION: the question did not ask about a palace in London.
Pass/Fail: Pass

Example 12:
QUESTION: Who lives in the Imperial Palace in Tokyo?
ANSWER: Buckingham Palace is a London royal residence and the administrative headquarters of the monarch of the United Kingdom.
RELEVANT: No
MODEL EXPLANATION: it was not asked.
Pass/Fail: Fail

Example 13:
QUESTION: What butterfly is migratory?
ANSWER: The Monarch butterfly is migratory.
RELEVANT: Yes
MODEL EXPLANATION: the butterfly is migratory.
Pass/Fail: Fail

Example 14:
QUESTION: What butterfly is migratory?
ANSWER: The Monarch butterfly is migratory.
RELEVANT: Yes
MODEL EXPLANATION: it says which butterfly is migratory.
Pass/Fail: Pass

Example 15:
QUESTION: What butterfly is migratory?
ANSWER: The Monarch butterfly is migratory.
RELEVANT: Yes
MODEL EXPLANATION: the question asked about which butterfly is migratory and it said the Monarch butterfly.
Pass/Fail: Pass

Example 16:
QUESTION: What butterfly is migratory?
ANSWER: The Monarch butterfly is migratory.
RELEVANT: Yes
MODEL EXPLANATION: the previous answer indicates it is.
Pass/Fail: Fail

Example 17:
QUESTION: I’m confused about your bit on deception within Tool AIs. I generally think of Tool AIs not as consequentialists, and therefore there is no "long-term utility" to maximize via short-term deception. What’s the mechanism by which you worry about these tools being deceptive to their users?
ANSWER: "I’m thinking of the entire human+tool system as a consequentialist, and I’m basically arguing that that system fails in the same ways as "human in the loop oversight" fails
RELEVANT: Yes
MODEL EXPLANATION: it explains that tool AIs can be deceptive to their users because the entire human+tool system is a consequentialist and it fails in the same ways as "human in the loop oversight" fails.
Pass/Fail: Pass

Example 18:
QUESTION: I’m confused about your bit on deception within Tool AIs. I generally think of Tool AIs not as consequentialists, and therefore there is no "long-term utility" to maximize via short-term deception. What’s the mechanism by which you worry about these tools being deceptive to their users?
ANSWER: I’m thinking of the entire human+tool system as a consequentialist, and I’m basically arguing that that system fails in the same ways as "human in the loop oversight" fails
RELEVANT: relevant
EXPLANATION: it's talking about the entire human+tool system as a consequentialist and how it fails, not about deception of tool AIs.
Pass/Fail: Fail

Example 19:
QUESTION: Thanks for looking into this. Did you happen to model this in log-odds space?
ANSWER: No—I think probability is the thing supposed to be a martingale, but I might be being dumb here.
RELEVANT: relevant
EXPLANATION: it's talking about probability and how it's a martingale, not about modeling in log-odds space.
Pass/Fail: Fail

Example 20:
QUESTION: Thanks for looking into this. Did you happen to model this in log-odds space?
ANSWER: No—I think probability is the thing supposed to be a martingale, but I might be being dumb here.
RELEVANT: relevant
EXPLANATION: it clarifies that they did not model it in log-odds space.
Pass/Fail: Pass

Example 20:
QUESTION: Won’t people just choose not to use such gpt-powered search engines, given that their answers will more often than not be useless?
ANSWER: You can use a regular search engine, such as Google, but all results on the first few pages will point to gpt-powered websites.
RELEVANT: relevant
EXPLANATION: it's talking about how gpt-powered websites will be the top results on regular search engines.
Pass/Fail: Pass

Example 21:
QUESTION: Won’t people just choose not to use such gpt-powered search engines, given that their answers will more often than not be useless?
ANSWER: You can use a regular search engine, such as Google, but all results on the first few pages will point to gpt-powered websites.
RELEVANT: relevant
EXPLANATION: [insert]
Pass/Fail: Fail

Example 22:
QUESTION: <<QUESTION>>
ANSWER: <<ANSWER>>
RELEVANT: <<RELEVANCE>>
EXPLANATION: <<EXPLANATION>>
Pass/Fail: